# -*- coding: utf-8 -*-
"""Diabetes Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JwvdhIhqoujkoHPkmRUZGFETZ6HcdU4H

# Data Engineer

## Bağlantı
"""

!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse
from google.colab import auth
auth.authenticate_user()
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

!mkdir -p drive
!google-drive-ocamlfuse drive
!ls

"""## Modul"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

"""## Dataset import"""

df = pd.read_csv('/content/drive/Colab Notebooks/diabetes/data.csv')
df.head()

"""# Data analysis

## Explorer datasets
"""

df.shape

df.dtypes

df.info()

"""### Statistical summary"""

df.describe()

"""## Data Cleaning"""

df.shape

df=df.drop_duplicates()

df.shape

"""### Find the null values """

df.isnull().sum()

df.columns

"""### Check the no of zero values in dataset"""

print('No. of zero values in Glucose',df[df['Glucose']==0].shape[0])

print('No. of zero values in BloodPressure',df[df['BloodPressure']==0].shape[0])

print('No. of zero values in SkinThickness',df[df['SkinThickness']==0].shape[0])

print('No. of zero values in Insulin',df[df['Insulin']==0].shape[0])

print('No. of zero values in BMI',df[df['BMI']==0].shape[0])

"""### Replace no of zero vallues with mean of that columns"""

df.columns

df['Glucose']=df['Glucose'].replace(0,df['Glucose'].mean())
print('No. of zero values in Glucose', df[df['Glucose']==0].shape[0])

df['BloodPressure']=df['BloodPressure'].replace(0,df['BloodPressure'].mean())
df['SkinThickness']=df['SkinThickness'].replace(0,df['SkinThickness'].mean())
df['Insulin']=df['Insulin'].replace(0,df['Insulin'].mean())
df['BMI']=df['BMI'].replace(0,df['BMI'].mean())

df.describe()

"""## Data Visualization"""

f,ax=plt.subplots(1,2,figsize=(15,10))
df['Outcome'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)
ax[0].set_title('Outcome')
ax[0].set_ylabel('')
sns.countplot('Outcome',data=df,ax=ax[1])
ax[1].set_title('Outcome')
N,P = df['Outcome'].value_counts()
print('Negative (0): ', N)
print('Positive (1): ',P)
plt.grid()
plt.show()

"""### Histograms"""

df.hist(bins=10,figsize=(10,10))
plt.show()

"""### Scatter plot"""

from pandas.plotting import scatter_matrix
scatter_matrix(df, figsize=(20,20));

"""### Pairplot"""

sns.pairplot(data=df, hue='Outcome')
plt.show()

"""### Analyzing relationships between variables"""

corrmat = df.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(10,10))
g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap='RdYlGn')

"""# Data Science

## Split the data frame into X & y
"""

target_name = 'Outcome'

y= df[target_name]
X= df.drop(target_name, axis=1)

X.head()

y.head()

"""## Apply Feature Scaling"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X)
SSX = scaler.transform(X)

"""## Train - Test split"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(SSX, y, test_size=0.2, random_state=42)

X_test.shape,y_test.shape

X_train.shape,y_train.shape

"""## Build the CLASSIFICATION Algorithms

### Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(solver = 'liblinear', multi_class='ovr')
lr.fit(X_train, y_train)

"""### KNeighborsClssifler (KNN)"""

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(X_train,y_train)

"""### Navie-Bayes Classifier"""

from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(X_train, y_train)

"""### Support Vector Machine (SVM)"""

from sklearn.svm import SVC
sv=SVC()
sv.fit(X_train, y_train)

"""### Decision tree"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(criterion='entropy')
rf.fit(X_train,y_train)

"""## Prediction

### Logistic Regression
"""

X_test.shape

lr_pred=lr.predict(X_test)

lr_pred.shape

"""### KNN"""

knn_pred=knn.predict(X_test)

"""### Naivie Bayes"""

nb_pred=nb.predict(X_test)

"""### Support Vector Machine (SVM)"""

sv_pred=sv.predict(X_test)

"""### Decesion Tree"""

dt_pred=dt.predict(X_test)

"""### Random Forest"""

rf_pred=rf.predict(X_test)

"""## Model Evaluation

### Train Score and Test Score
"""

# Logistic Regression
from sklearn.metrics import accuracy_score
print('Train Accuracy of Logistic Regression',lr.score(X_train,y_train)*100)
print('Accuracy (TEST) score of Logistic Regression',lr.score(X_test,y_test)*100)
print('Accuracy (TEST) score of Logistic Regression',accuracy_score(y_test,lr_pred)*100)

# KNN
print('Train Accuracy of K-Nearest Neighbors',knn.score(X_train,y_train)*100)
print('Accuracy (TEST) score of K-Nearest Neighbors',knn.score(X_test,y_test)*100)
print('Accuracy (TEST) score of K-Nearest Neighbors',accuracy_score(y_test,knn_pred)*100)

# Navie-Bayes
print('Train Accuracy of Naive Bayes classifier',nb.score(X_train,y_train)*100)
print('Accuracy (TEST) Naive Bayes classifier',nb.score(X_test,y_test)*100)
print('Accuracy (TEST) Naive Bayes classifier',accuracy_score(y_test,nb_pred)*100)

# SVM
print('Train Accuracy of Support Vector Classification',sv.score(X_train,y_train)*100)
print('Accuracy (TEST) score of Support Vector Classification',sv.score(X_test,y_test)*100)
print('Accuracy (TEST) score of Support Vector Classification',accuracy_score(y_test,sv_pred)*100)

# Decesion Tree
print('Train Accuracy of Decision Tree',dt.score(X_train,y_train)*100)
print('Accuracy (TEST) score of Decision Tree',dt.score(X_test,y_test)*100)
print('Accuracy (TEST) score of Decision Tree',accuracy_score(y_test,dt_pred)*100)

# Random Forest
print('Train Accuracy of Random forest',rf.score(X_train,y_train)*100)
print('Accuracy (TEST) score of Random forest',rf.score(X_test,y_test)*100)
print('Accuracy (TEST) score of Random forest',accuracy_score(y_test,rf_pred)*100)

"""### Confusion Matrix"""

from sklearn.metrics import classification_report,confusion_matrix
cm=confusion_matrix(y_test,lr_pred)
cm

sns.heatmap(confusion_matrix(y_test,lr_pred),annot=True,fmt='d')

TN= cm[0,0]
FP= cm[0,1]
FN= cm[1,0]
TP= cm[1,1]

TN, FP, FN, TP

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve
cm = confusion_matrix(y_test, lr_pred)

print('TN - True Negative {}'.format(cm[0,0]))
print('TN - False Positive {}'.format(cm[0,1]))
print('TN - False Negative {}'.format(cm[1,0]))
print('TN - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))*100))
print('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))*100))

plt.clf()
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)
classNames=['0','1']
plt.title('Confusion Matrix of Logistic Regression')
plt.ylabel('Actual(True) values')
plt.xlabel('Predicted values')
tick_marks = np.arange(len(classNames))
plt.xticks(tick_marks, classNames, rotation=45)
plt.yticks(tick_marks, classNames)
s = [['TN', 'FP'],['FN','TP']]
for i in range(2):
    for j in range(2):
        plt.text(j,i, str(s[i][j]+'='+str(cm[i][j])))
plt.show()

pd.crosstab(y_test, lr_pred, margins=False)

pd.crosstab(y_test, lr_pred, margins=True)

pd.crosstab(y_test, lr_pred, rownames=['Actual values'], 
            colnames=['Predicted values'], margins=True)

"""### Precision(PPV-Positive Predictive Value)"""

TP,FP

Precision=TP/(TP+FP)
Precision

35/(35+16)

precision_Score = TP / float(TP + FP)*100
print('Precision score : {0:0.4f}'.format(precision_Score))

from sklearn.metrics import precision_score
print('precision Score is:', precision_score(y_test,lr_pred)*100)
print('Micro Average precision Score is:', precision_score(y_test,lr_pred,average='micro')*100)
print('Macro Average precision Score is:', precision_score(y_test,lr_pred,average='macro')*100)
print('Weighted Average precision Score is:', precision_score(y_test,lr_pred,average='weighted')*100)
print('precision Score on Non weighted score is:', precision_score(y_test,lr_pred, average=None)*100)

print('Classification Report of Logistic Regression: \n', classification_report(y_test,lr_pred,digits=4))

"""### False Positive Rate (FPR)"""

FPR = FP / float(FP+TN)*100
print('False Positive Rate: {0:0.4f}'.format(FPR))

FP, TN

16/(16+83)

"""### Specificity"""

specificity = TN/(TN+FP)*100
print('Specificity : {0:0.4f}'.format(specificity))

"""### F1-Score"""

from sklearn.metrics import f1_score
print('f1_score of macro :',f1_score(y_test, lr_pred)*100)

print('Micro Average F1_Score is:', f1_score(y_test,lr_pred,average='micro')*100)
print('Macro Average F1_Score is:', f1_score(y_test,lr_pred,average='macro')*100)
print('Weighted Average F1_Score is:', f1_score(y_test,lr_pred,average='weighted')*100)
print('F1_Score on Non weighted is:', f1_score(y_test,lr_pred, average=None)*100)

"""### Classification Report of Logistic Regression"""

from sklearn.metrics import classification_report
print('Classification Report of Logistic Regression: \n', classification_report(y_test,lr_pred,digits=4))

"""## ROC Curve and ROC AUC

### Confusion Logistic Regression
"""

auc = roc_auc_score(y_test, lr_pred)
print('ROC AUC SCORE of Logistic Regression is',auc)

fpr, tpr, thresholds = roc_curve(y_test, lr_pred)
plt.plot(fpr, tpr, color='orange', label='ROC')
plt.plot([0,1],[0,1], color='darkblue', linestyle='--',label='ROC curve (area = %0.2f)'% auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve of Logistic Regression')
plt.legend()
plt.show()

sns.heatmap(confusion_matrix(y_test,rf_pred),annot=True,fmt='d')

"""### Confusion Matrix of KNN"""

sns.heatmap(confusion_matrix(y_test,knn_pred),annot=True,fmt='d')

cm = confusion_matrix(y_test, knn_pred)

print('TN - True Negative {}'.format(cm[0,0]))
print('TN - False Positive {}'.format(cm[0,1]))
print('TN - False Negative {}'.format(cm[1,0]))
print('TN - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))*100))
print('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))*100))

print('Classification Report of KNN: \n', classification_report(y_test,knn_pred,digits=4))

"""### Are Under Curve of KNN"""

auc = roc_auc_score(y_test, knn_pred)
print('ROC AUC SCORE of KNN is',auc)

fpr, tpr, thresholds = roc_curve(y_test, knn_pred)
plt.plot(fpr, tpr, color='orange', label='ROC')
plt.plot([0,1],[0,1], color='darkblue', linestyle='--',label='ROC curve (area = %0.2f)'% auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve of KNN')
plt.legend()
plt.show()

"""### Confusion Matrix of Naive Bayes"""

sns.heatmap(confusion_matrix(y_test,nb_pred),annot=True,fmt='d')

cm = confusion_matrix(y_test, nb_pred)

print('TN - True Negative {}'.format(cm[0,0]))
print('TN - False Positive {}'.format(cm[0,1]))
print('TN - False Negative {}'.format(cm[1,0]))
print('TN - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))*100))
print('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))*100))

print('Classification Report of Naive Bayes : \n', classification_report(y_test,knn_pred,digits=4))

"""### Are Under Curve of Naive Bayes"""

auc = roc_auc_score(y_test, knn_pred)
print('ROC AUC SCORE of Naive Bayes is',auc)

fpr, tpr, thresholds = roc_curve(y_test, nb_pred)
plt.plot(fpr, tpr, color='orange', label='ROC')
plt.plot([0,1],[0,1], color='darkblue', linestyle='--',label='ROC curve (area = %0.2f)'% auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve of Naive Bayes')
plt.legend()
plt.show()

"""### Confusion Matrix of SVM"""

sns.heatmap(confusion_matrix(y_test,sv_pred),annot=True,fmt='d')

cm = confusion_matrix(y_test, sv_pred)

print('TN - True Negative {}'.format(cm[0,0]))
print('TN - False Positive {}'.format(cm[0,1]))
print('TN - False Negative {}'.format(cm[1,0]))
print('TN - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))*100))
print('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))*100))

print('Classification Report of SVM : \n', classification_report(y_test,sv_pred,digits=4))

"""### Are Under Curve of SVM"""

auc = roc_auc_score(y_test, sv_pred)
print('ROC AUC SCORE of SVM is',auc)

fpr, tpr, thresholds = roc_curve(y_test, sv_pred)
plt.plot(fpr, tpr, color='orange', label='ROC')
plt.plot([0,1],[0,1], color='darkblue', linestyle='--',label='ROC curve (area = %0.2f)'% auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve of SVM')
plt.legend()
plt.show()

"""### Confusion Matrix of Decesion Tree"""

sns.heatmap(confusion_matrix(y_test,dt_pred),annot=True,fmt='d')

cm = confusion_matrix(y_test, dt_pred)

print('TN - True Negative {}'.format(cm[0,0]))
print('TN - False Positive {}'.format(cm[0,1]))
print('TN - False Negative {}'.format(cm[1,0]))
print('TN - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))*100))
print('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))*100))

print('Classification Report of Decesion Tree: \n', classification_report(y_test,dt_pred,digits=4))

"""### Are Under Curve of Decesion Tree"""

auc = roc_auc_score(y_test, dt_pred)
print('ROC AUC SCORE of Decesion Tree is',auc)

fpr, tpr, thresholds = roc_curve(y_test, dt_pred)
plt.plot(fpr, tpr, color='orange', label='ROC')
plt.plot([0,1],[0,1], color='darkblue', linestyle='--',label='ROC curve (area = %0.2f)'% auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve of Decesion Tree')
plt.legend()
plt.show()

"""### Confusion Matrix of Random Forest"""

sns.heatmap(confusion_matrix(y_test,rf_pred),annot=True,fmt='d')

cm = confusion_matrix(y_test, rf_pred)

print('TN - True Negative {}'.format(cm[0,0]))
print('TN - False Positive {}'.format(cm[0,1]))
print('TN - False Negative {}'.format(cm[1,0]))
print('TN - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))*100))
print('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))*100))

print('Classification Report of Random Forest: \n', classification_report(y_test,rf_pred,digits=4))

"""### Are Under Curve of Random Forest"""

auc = roc_auc_score(y_test, rf_pred)
print('ROC AUC SCORE of Decesion Tree is',auc)

fpr, tpr, thresholds = roc_curve(y_test, rf_pred)
plt.plot(fpr, tpr, color='orange', label='ROC')
plt.plot([0,1],[0,1], color='darkblue', linestyle='--',label='ROC curve (area = %0.2f)'% auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve of Random Forest')
plt.legend()
plt.show()